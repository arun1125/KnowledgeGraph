[[BERT]]
# Self Attention
Papers
- Attention is all you need


What is it? 
The idea is that we can put weights to the words in the input sequence according to their relevance/contribution to understanding of the word whose represnetation is being made 

from our entitiy linking example

![[Pasted image 20220416134305.png]]
