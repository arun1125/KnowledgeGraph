# Ridge Regression
# Intuitive Explanation
In regular [[Linear Regression]] the error can be large due to the variance so to reduce this variance at a slight cost in bias we add a [[Regularization]] term. The reduction in variance compensates for the increase in bias so the overall error improves

Also when you have a large number of features you can find the features that matter the most
 
# Technical Explanation
https://www.youtube.com/watch?v=Q81RR3yKn30

# Related 
[[Regularization]]
[[Linear Regression]]
[[PCA]]
[[SVD]]
[[Feature Selection]]
[[Lasso Regression]]
[[Elastic Net]]


# References


https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com

https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues

https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca

https://stats.stackexchange.com/search?tab=votes&q=ridge%20regression

https://stats.stackexchange.com/questions/866/when-should-i-use-lasso-vs-ridge

https://stats.stackexchange.com/questions/151304/why-is-ridge-regression-called-ridge-why-is-it-needed-and-what-happens-when/151351#151351

https://stats.stackexchange.com/questions/137057/ridge-penalized-glms-using-row-augmentation/137072#137072